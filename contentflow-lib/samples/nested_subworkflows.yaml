# Nested Subworkflows - Multi-Level Parallelism
#
# Process batch of documents, each split into chunks, with parallel execution at each level.
# Pattern: Batch → Documents (parallel) → Chunks (parallel) → Aggregate

connectors:
  - name: storage
    type: blob_storage
    settings:
      account_name: ${STORAGE_ACCOUNT}
  
  - name: doc_intelligence
    type: document_intelligence
    settings:
      endpoint: ${DOCUMENT_INTELLIGENCE_ENDPOINT}
  
  - name: ai_model
    type: ai_inference
    settings:
      endpoint: ${AI_ENDPOINT}
  
  - name: search
    type: ai_search
    settings:
      endpoint: ${SEARCH_ENDPOINT}
      index_name: documents

# Level 3: Process individual chunk (innermost)
subworkflows:
  - name: process_chunk
    description: Analyze a single document chunk
    
    executors:
      - id: extract_chunk_content
        type: document_intelligence_extractor
        settings:
          model_id: prebuilt-layout
          extract_text: true
          extract_tables: true
        connectors: [doc_intelligence]
      
      - id: analyze_chunk_content
        type: custom_ai_prompt
        settings:
          system_prompt: |
            Analyze this chunk:
            - Extract named entities
            - Identify key topics
            - Rate importance (1-10)
          user_prompt: |
            Chunk {chunk_id} (pages {page_range}):
            {content}
          max_completion_tokens: 400
        connectors: [ai_model]
    
    execution_sequence: [extract_chunk_content, analyze_chunk_content]

# Level 2: Process individual document (uses chunk subworkflow)
subworkflows:
  - name: process_document
    description: Split document and process chunks in parallel
    
    executors:
      # Split document into chunks
      - id: split_into_chunks
        type: document_splitter
        settings:
          split_by: pages
          chunk_size: 3        # 3 pages per chunk
          overlap_pages: 1     # 1 page overlap
        connectors: []
      
      # Process chunks in parallel (Level 3)
      - id: process_chunks_parallel
        type: chunk_processor
        settings:
          subworkflow: process_chunk  # Nested subworkflow!
          max_parallel: 5             # 5 chunks at a time
          chunk_field: chunks
        connectors: []
      
      # Aggregate chunk results for this document
      - id: aggregate_chunks
        type: result_aggregator
        settings:
          aggregation_strategy: merge
          merge_fields: [entities, topics]
          compute_stats:
            - field: importance_score
              operations: [avg, max]
        connectors: []
      
      # Create document-level summary
      - id: summarize_document
        type: custom_ai_prompt
        settings:
          system_prompt: "Create executive summary from chunk analyses."
          user_prompt: |
            Document: {document_id}
            Chunk analyses: {chunk_results}
          max_completion_tokens: 500
        connectors: [ai_model]
    
    execution_sequence: [split_into_chunks, process_chunks_parallel, aggregate_chunks, summarize_document]

# Level 1: Main workflow (processes batch of documents)
workflows:
  - name: batch_nested_processing
    description: Process batch with nested parallelism (batch → docs → chunks)
    
    executors:
      # Load batch of documents
      - id: load_batch
        type: batch_loader
        settings:
          source_container: input-docs
          batch_size: 10           # Load 10 documents
          file_pattern: "*.pdf"
        connectors: [storage]
      
      # Pre-filter documents
      - id: filter_batch
        type: document_filter
        settings:
          filters:
            - field: file_size
              operator: less_than
              value: 10485760      # Max 10MB
            - field: page_count
              operator: greater_than
              value: 1             # At least 2 pages
        connectors: []
      
      # Process documents in parallel (Level 2)
      - id: process_documents_parallel
        type: document_batch_processor
        settings:
          subworkflow: process_document  # Uses nested chunk processing!
          max_parallel: 3                # 3 documents at a time
          document_field: documents
          timeout_seconds: 600           # 10 min per document
          continue_on_error: true        # Don't fail batch on single doc error
        connectors: []
      
      # Aggregate all document results
      - id: aggregate_batch
        type: result_aggregator
        settings:
          aggregation_strategy: summarize
          summary_fields:
            - total_documents
            - total_chunks
            - avg_importance_score
          compute_stats:
            - field: page_count
              operations: [sum, avg]
            - field: entity_count
              operations: [sum, avg]
          include_all_results: true
        connectors: []
      
      # Create batch-level insights
      - id: batch_insights
        type: custom_ai_prompt
        settings:
          system_prompt: |
            Analyze batch of processed documents:
            - Common themes across documents
            - Most important findings
            - Recommendations for next steps
          user_prompt: |
            Batch results:
            Total docs: {total_documents}
            Summaries: {document_summaries}
          max_completion_tokens: 800
        connectors: [ai_model]
      
      # Index all results
      - id: index_batch
        type: ai_search_index_writer
        settings:
          index_mode: chunks      # Index at chunk level
          chunk_field: all_chunks
          metadata_fields:
            - document_id
            - chunk_id
            - page_range
            - entities
            - topics
            - importance_score
          batch_size: 100
        connectors: [search]
    
    execution_sequence: [
      load_batch,
      filter_batch,
      process_documents_parallel,
      aggregate_batch,
      batch_insights,
      index_batch
    ]

# Execution visualization:
#
# load_batch
#    ↓
# filter_batch
#    ↓
# process_documents_parallel (Level 1: 3 docs in parallel)
#    ├─→ Document 1
#    │      ↓
#    │   split_into_chunks
#    │      ↓
#    │   process_chunks_parallel (Level 2: 5 chunks in parallel)
#    │      ├─→ Chunk 1 → extract → analyze
#    │      ├─→ Chunk 2 → extract → analyze
#    │      ├─→ Chunk 3 → extract → analyze
#    │      ├─→ Chunk 4 → extract → analyze
#    │      └─→ Chunk 5 → extract → analyze
#    │      ↓
#    │   aggregate_chunks
#    │      ↓
#    │   summarize_document
#    │
#    ├─→ Document 2 (same flow)
#    │
#    └─→ Document 3 (same flow)
#    ↓
# aggregate_batch
#    ↓
# batch_insights
#    ↓
# index_batch

# Parallelism Levels:
# - Level 1 (Batch): 3 documents processed simultaneously
# - Level 2 (Document): 5 chunks per document simultaneously
# - Level 3 (Chunk): Sequential extract → analyze
#
# Maximum concurrent chunk processing: 3 docs × 5 chunks = 15 chunks
